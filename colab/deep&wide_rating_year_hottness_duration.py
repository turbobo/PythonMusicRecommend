# -*- coding: utf-8 -*-
"""deep&wide_rating_year_hottness_duration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sPSh06OXPM_gAvwkgRVh8AnW0F5XM-UJ
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import torch

from collections import namedtuple
from itertools import chain
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import sqlite3

import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import seaborn as sns

import matplotlib.pyplot as plt
# %matplotlib inline

#Load data
path = './drive/MyDrive/data/metadata/'

#Ratings
# ratings = pd.read_csv(path+'ratings.dat', sep='::', header=None, engine='python')
# ratings.columns = ['userId','movieId','rating','timestamp']
# ratings = ratings.drop('timestamp', axis=1)
# ratings = pd.read_csv(path+'user_item_rating_2.csv')   #, sep=',', header=None, engine='python')
ratings = pd.read_csv(path+'user_item_rating_all_200w.csv')   #, sep=',', header=None, engine='python')
ratings.columns = ['user','song','rating']
ratings.info(verbose=True, max_cols=True, memory_usage=True, null_counts=True)

# data
# data = pd.read_csv(path+'track_200w.csv')
data = pd.read_csv(path+'track_all_200w.csv')
# data = data[['user','song','play_count','year','tags']]
data.info(verbose=True, max_cols=True, memory_usage=True, null_counts=True)
# data.head(10)
data.astype({'user': 'int32', 'song': 'int32', 'play_count': 'int32', 'year': 'int32'})

# # 字典user_playcounts记录每个用户的播放总量
# user_playcounts = {}
# for user, group in data.groupby('user'):
#     user_playcounts[user] = group['play_count'].sum()
# temp_user = [user for user in user_playcounts.keys() if user_playcounts[user] > 100]
# temp_playcounts = [playcounts for user, playcounts in user_playcounts.items() if playcounts > 100]
# # data = data[data.user.isin(temp_user)]
# ratings = ratings[ratings.user.isin(temp_user)]
# print('歌曲播放量大于100的用户数量占总体用户数量的比例为', str(round(len(temp_user)/len(user_playcounts), 4)*100)+'%')
# print('歌曲播放量大于100的用户产生的播放总量占总体播放总量的比例为', str(round(sum(temp_playcounts) / sum(user_playcounts.values())*100, 4))+'%')
# print('歌曲播放量大于100的用户产生的数据占总体数据的比例为', str(round(len(data[data.user.isin(temp_user)])/len(data)*100, 4))+"%")



# song_playcounts字典，记录每首歌的播放量
# song_playcounts = {}
# for song, group in data.groupby('song'):
#     song_playcounts[song] = group['play_count'].sum()
# temp_song = [song for song in song_playcounts.keys() if song_playcounts[song] > 50]
# temp_playcounts = [playcounts for song, playcounts in song_playcounts.items() if playcounts > 50]
# # data = data[data.song.isin(temp_song)]
# ratings = ratings[ratings.song.isin(temp_song)]
# print('播放量大于50的歌曲数量占总体歌曲数量的比例为', str(round(len(temp_song)/len(song_playcounts), 4)*100)+'%')
# print('播放量大于50的歌曲产生的播放总量占总体播放总量的比例为', str(round(sum(temp_playcounts) / sum(song_playcounts.values())*100, 4))+'%')
# print('播放量大于50的歌曲产生的数据占总体数据的比例为', str(round(len(data[data.song.isin(temp_song)])/len(data)*100, 4))+"%")

# data['duration']=data['duration'].apply(lambda x : int(x))
# data.astype({'duration': 'int32'})
print("data:****************************************")
data.info(verbose=True, max_cols=True, memory_usage=True, null_counts=True)


#songs
conn = sqlite3.connect(path+'track_metadata.db')
cur = conn.cursor()
cur.execute("SELECT name FROM sqlite_master WHERE type='table'")
cur.fetchall()
#
# # 获得数据的dataframe
track_metadata_df = pd.read_sql(con=conn, sql='select * from songs')
track_metadata_df = track_metadata_df[['track_id','duration']]

songs = pd.merge(data,track_metadata_df,how='inner',on="track_id")
songs = songs[['song','artist_hotttnesss','year','duration']]
songs = songs.rename(columns={'year': 'song_year'})
# 根据songID去重
songs.drop_duplicates(subset=['song'],keep='first',inplace=True)
# 根据year去除为空，去除为0
songs = songs.dropna(subset=['song_year'])
songs = songs[songs.song_year != 0]
print('根据songID去重，去0***********************')
songs.info(verbose=True, max_cols=True, memory_usage=True, null_counts=True)
# 去重artist_hotttnesss为空，为0
songs = songs.dropna(subset=['artist_hotttnesss'])
songs = songs[songs.artist_hotttnesss != 0]
print('去重artist_hotttnesss为空，为0***********************')
songs.info(verbose=True, max_cols=True, memory_usage=True, null_counts=True)

# 去重duration为空，为0
songs = songs.dropna(subset=['duration'])
songs = songs[songs.duration != 0]
print('duration***********************')
songs.info(verbose=True, max_cols=True, memory_usage=True, null_counts=True)



# 评分中year为0的歌曲去除
song_with_year = [song for song in songs.song]
ratings = ratings[ratings.song.isin(song_with_year)]
del(song_with_year)

songs.head()

# song_with_year.head(10)
# song_with_year.info(verbose=True, max_cols=True, memory_usage=True, null_counts=True)
#Users
# users = pd.read_csv(path+'users.dat', sep='::', header=None, engine='python')
# users.columns = ['userId','Gender','Age','Occupation','Zip-code']
# users = users.drop('Zip-code', axis=1)
# user没有附加信息
# users = data[['user']]

#Data quality
print('Duplicated rows in ratings file: ' + str(ratings.duplicated().sum()))

# n_users = ratings.userId.unique().shape[0]
# n_movies = ratings.movieId.unique().shape[0]
n_users = ratings.user.unique().shape[0]
n_songs = ratings.song.unique().shape[0]

print('Number of users: {}'.format(n_users))
print('Number of songs: {}'.format(n_songs))
print('Sparsity: {:4.3f}%'.format(float(ratings.shape[0]) / float(n_users*n_songs) * 100))

# all


# user>100


# song>50


# user>100 and song>50

ratings.shape
print('ratings*************************')
ratings.head(10)
print('songs*************************')
songs.head(10)

# 评分和歌曲连接
# final_df = ratings.merge(songs, left_on='song', right_on='song', how='inner')
# final_df = pd.merge(ratings,songs,how='left',on="song")
final_df = pd.merge(ratings,songs,how='inner',on="song")
# user、song不按照播放量筛选：1648992条数据 
final_df.info(verbose=True, max_cols=True, memory_usage=True, null_counts=True)

n_users = final_df.user.unique().shape[0]
n_songs = final_df.song.unique().shape[0]

print('Number of users: {}'.format(n_users))
print('Number of songs: {}'.format(n_songs))
print('Sparsity: {:4.3f}%'.format(float(final_df.shape[0]) / float(n_users*n_songs) * 100))

final_df.head(10)  

# 释放内存
del(songs)
del(ratings)
del(track_metadata_df)

"""### 数据处理"""

def encoder(df, cols=None):
    if cols == None:
        cols = list(df.select_dtypes(include=['object']).columns)

    val_types = dict()
    for c in cols:
        val_types[c] = df[c].unique()

    val_to_idx = dict()
    for k, v in val_types.items():
        val_to_idx[k] = {o: i for i, o in enumerate(val_types[k])}

    for k, v in val_to_idx.items():
        df[k] = df[k].apply(lambda x: v[x])

    return val_to_idx, df

def data_processing(df, wide_cols, embeddings_cols, continuous_cols, target,
                    scale=False, def_dim=8):


    if type(embeddings_cols[0]) is tuple:
        emb_dim = dict(embeddings_cols)
        embeddings_cols = [emb[0] for emb in embeddings_cols]
    else:
        emb_dim = {e:def_dim for e in embeddings_cols}
    deep_cols = embeddings_cols+continuous_cols

    # Extract the target and copy the dataframe so we don't mutate it
    # internally.
    Y = np.array(df[target])
    all_columns = list(set(wide_cols + deep_cols ))
    df_tmp = df.copy()[all_columns]


    # 提取可在以后进行热编码的分类列名
    categorical_columns = list(df_tmp.select_dtypes(include=['object']).columns)


    encoding_dict,df_tmp = encoder(df_tmp)
    encoding_dict = {k:encoding_dict[k] for k in encoding_dict if k in deep_cols}
    embeddings_input = []
    for k,v in encoding_dict.items():
        embeddings_input.append((k, len(v), emb_dim[k]))

    df_deep = df_tmp[deep_cols]
    deep_column_idx = {k:v for v,k in enumerate(df_deep.columns)}


    if scale:
        scaler = StandardScaler()
        for cc in continuous_cols:
            df_deep[cc]  = scaler.fit_transform(df_deep[cc].values.reshape(-1,1))

    df_wide = df_tmp[wide_cols]
    del(df_tmp)
    dummy_cols = [c for c in wide_cols if c in categorical_columns]
    df_wide = pd.get_dummies(df_wide, columns=dummy_cols)

    # 和DeepFM参数一致
    X_train_deep, X_test_deep = train_test_split(df_deep.values, test_size=0.2, random_state=1024)
    X_train_wide, X_test_wide = train_test_split(df_wide.values, test_size=0.2, random_state=1024)
    y_train, y_test = train_test_split(Y, test_size=0.2, random_state=2021)
    # X_train_deep, X_test_deep = train_test_split(df_deep.values, test_size=0.3, random_state=1463)
    # X_train_wide, X_test_wide = train_test_split(df_wide.values, test_size=0.3, random_state=1463)
    # y_train, y_test = train_test_split(Y, test_size=0.3, random_state=1981)

    group_dataset = dict()
    train_dataset = namedtuple('train_dataset', 'wide, deep, labels')
    test_dataset  = namedtuple('test_dataset' , 'wide, deep, labels')
    group_dataset['train_dataset'] = train_dataset(X_train_wide, X_train_deep, y_train)
    group_dataset['test_dataset']  = test_dataset(X_test_wide, X_test_deep, y_test)
    group_dataset['embeddings_input']  = embeddings_input
    group_dataset['deep_column_idx'] = deep_column_idx
    group_dataset['encoding_dict'] = encoding_dict

    return group_dataset

"""### 数据设置"""

#数据设置
# wide_cols = ['movie_year','gender','age', 'occupation','genres','userId','movieId']
# embeddings_cols = [('tags',20), ('user',100), ('song',100)]
wide_cols = ['song_year','artist_hotttnesss','duration','user','song']
embeddings_cols = [('user',100), ('song',100)]
crossed_cols = ()
continuous_cols = ['song_year','artist_hotttnesss','duration']
target = 'rating'

#拆分数据并生成嵌入
data_processed = data_processing(
    final_df, wide_cols,
    embeddings_cols,
    continuous_cols,
    target,
    scale=True)

use_cuda = torch.cuda.is_available()

#加载数据集
class DatasetLoader(Dataset):
    def __init__(self, data):

        self.X_wide = data.wide
        self.X_deep = data.deep
        self.Y = data.labels

    def __getitem__(self, idx):

        xw = self.X_wide[idx]
        xd = self.X_deep[idx]
        y  = self.Y[idx]

        return xw, xd, y

    def __len__(self):
        return len(self.Y)

"""### 类定义广度和深度神经网络"""

#类定义广度和深度神经网络
class NeuralNet(nn.Module):

    def __init__(self,
                 wide_dim,
                 embeddings_input,
                 continuous_cols,
                 deep_column_idx,
                 hidden_layers,
                 dropout,
                 encoding_dict,
                 n_class):

        super(NeuralNet, self).__init__()
        self.wide_dim = wide_dim
        self.deep_column_idx = deep_column_idx
        self.embeddings_input = embeddings_input
        self.continuous_cols = continuous_cols
        self.hidden_layers = hidden_layers
        self.dropout = dropout
        self.encoding_dict = encoding_dict
        self.n_class = n_class
        self.loss_values=[]

        # 创建要穿过深侧的嵌入层
        for col,val,dim in self.embeddings_input:
            setattr(self, 'emb_layer_'+col, nn.Embedding(val, dim))

        # 如果指定，则使用下拉框构建深侧隐藏层
        input_emb_dim = np.sum([emb[2] for emb in self.embeddings_input])
        # input_emb_dim为float64
        input_emb_dim = int(input_emb_dim)
        self.linear_1 = nn.Linear(input_emb_dim+len(continuous_cols), self.hidden_layers[0])
        if self.dropout:
            self.linear_1_drop = nn.Dropout(self.dropout[0])
        for i,h in enumerate(self.hidden_layers[1:],1):
            setattr(self, 'linear_'+str(i+1), nn.Linear( self.hidden_layers[i-1], self.hidden_layers[i] ))
            if self.dropout:
                setattr(self, 'linear_'+str(i+1)+'_drop', nn.Dropout(self.dropout[i]))

        # 将模型的wide侧和deep侧连接到输出神经元
        self.output = nn.Linear(self.hidden_layers[-1]+self.wide_dim, self.n_class)

    # 学习率0.001
    def compile(self, optimizer="Adam", learning_rate=0.001, momentum=0.0):

        self.activation, self.criterion = None, F.mse_loss

        if optimizer == "Adam":
            self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)
        if optimizer == "RMSprop":
            self.optimizer = torch.optim.RMSprop(self.parameters(), lr=learning_rate)
        if optimizer == "SGD":
            self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=momentum)

        self.method = 'regression'


    def forward(self, X_w, X_d):

        # Deep 侧
        emb = [getattr(self, 'emb_layer_'+col)(X_d[:,self.deep_column_idx[col]].long())
               for col,_,_ in self.embeddings_input]
        if self.continuous_cols:
            cont_idx = [self.deep_column_idx[col] for col in self.continuous_cols]
            cont = [X_d[:, cont_idx].float()]
            deep_inp = torch.cat(emb+cont, 1)
        else:
            deep_inp = torch.cat(emb, 1)

        x_deep = F.relu(self.linear_1(deep_inp))
        if self.dropout:
            x_deep = self.linear_1_drop(x_deep)
        for i in range(1,len(self.hidden_layers)):
            x_deep = F.relu( getattr(self, 'linear_'+str(i+1))(x_deep) )
            if self.dropout:
                x_deep = getattr(self, 'linear_'+str(i+1)+'_drop')(x_deep)

        # Deep + Wide 侧
        wide_deep_input = torch.cat([x_deep, X_w.float()], 1)

        if not self.activation:
            out = self.output(wide_deep_input)
        else:
            out = self.activation(self.output(wide_deep_input))

        return out


    def fit(self, dataset, n_epochs, batch_size):

        widedeep_dataset = DatasetLoader(dataset)
        train_loader = torch.utils.data.DataLoader(dataset=widedeep_dataset,
                                                   batch_size=batch_size,
                                                   shuffle=True)

        # 将模型设置为评估模式，以便不应用退出
        net = self.train()
        for epoch in range(n_epochs):
            total=0
            correct=0
            for i, (X_wide, X_deep, target) in enumerate(train_loader):
                X_w = Variable(X_wide)
                X_d = Variable(X_deep)
                y = (Variable(target).float() if self.method != 'multiclass' else Variable(target))

                if use_cuda:
                    X_w, X_d, y = X_w.cuda(), X_d.cuda(), y.cuda()

                self.optimizer.zero_grad()
                y_pred =  net(X_w, X_d)
                y_pred = torch.squeeze(y_pred)
                loss = self.criterion(y_pred, y)
                loss.backward()
                self.optimizer.step()

                if self.method != "regression":
                    total+= y.size(0)
                    if self.method == 'logistic':
                        y_pred_cat = (y_pred > 0.5).squeeze(1).float()
                    if self.method == "multiclass":
                        _, y_pred_cat = torch.max(y_pred, 1)
                    correct+= float((y_pred_cat == y).sum().data[0])
            self.loss_values.append(loss.item())
            print ('Epoch {} of {}, Loss: {}'.format(epoch+1, n_epochs,
                                                     round(loss.item(),3)))


    def predict(self, dataset):


        X_w = Variable(torch.from_numpy(dataset.wide)).float()
        X_d = Variable(torch.from_numpy(dataset.deep))

        if use_cuda:
            X_w, X_d = X_w.cuda(), X_d.cuda()

        # set the model in evaluation mode so dropout is not applied
        net = self.eval()
        pred = net(X_w,X_d).cpu()
        if self.method == "regression":
            return pred.squeeze(1).data.numpy()
        if self.method == "logistic":
            return (pred > 0.5).squeeze(1).data.numpy()
        if self.method == "multiclass":
            _, pred_cat = torch.max(pred, 1)
            return pred_cat.data.numpy()




    def get_embeddings(self, col_name):
        params = list(self.named_parameters())
        emb_layers = [p for p in params if 'emb_layer' in p[0]]
        emb_layer  = [layer for layer in emb_layers if col_name in layer[0]][0]
        embeddings = emb_layer[1].cpu().data.numpy()
        col_label_encoding = self.encoding_dict[col_name]
        inv_dict = {v:k for k,v in col_label_encoding.items()}
        embeddings_dict = {}
        for idx,value in inv_dict.items():
            embeddings_dict[value] = embeddings[idx]

        return embeddings_dict

# 网络建立
wide_dim = data_processed['train_dataset'].wide.shape[1]
n_unique = len(np.unique(data_processed['train_dataset'].labels))
n_class = 1

deep_column_idx = data_processed['deep_column_idx']
embeddings_input= data_processed['embeddings_input']
encoding_dict   = data_processed['encoding_dict']
hidden_layers = [100,50]
dropout = [0.5,0.2]

use_cuda = torch.cuda.is_available()

model = NeuralNet(
    wide_dim,
    embeddings_input,
    continuous_cols,
    deep_column_idx,
    hidden_layers,
    dropout,
    encoding_dict,
    n_class)
model.compile(optimizer='Adam')
if use_cuda:
    model = model.cuda()

"""### 训练模型"""

#训练模型
train_dataset = data_processed['train_dataset']
model.fit(dataset=train_dataset, n_epochs=10, batch_size=256)

# 调参
# model.fit(dataset=train_dataset, n_epochs=30, batch_size=60)
# model.fit(dataset=train_dataset, n_epochs=50, batch_size=60)
# model.fit(dataset=train_dataset, n_epochs=80, batch_size=60)
# model.fit(dataset=train_dataset, n_epochs=100, batch_size=60)

#%%

test_dataset = data_processed['test_dataset']

#%%

y_pred = model.predict(dataset=test_dataset)

y = test_dataset.labels

#获取测试MSE
mean_squared_error(y_pred,y)

#获取测试MAE
mean_absolute_error(y_pred,y)

#可视化模型的训练误差
plt.plot(model.loss_values)
plt.ylabel('Train Error')
plt.xlabel('epoch')
plt.title("Train error for models initial settings")
plt.show()

"""### 超参数调谐  对于梯度下降算法"""

## 超参数调谐

### 对于梯度下降算法

models = []
methods = ["Adam","RMSprop"]

for method in methods:
    model = NeuralNet(
        wide_dim,
        embeddings_input,
        continuous_cols,
        deep_column_idx,
        hidden_layers,
        dropout,
        encoding_dict,
        n_class)
    model.compile(optimizer=method)
    if use_cuda:
        model = model.cuda()
    model.fit(dataset=train_dataset, n_epochs=5, batch_size=256)

    # 调参
    # model.fit(dataset=train_dataset, n_epochs=30, batch_size=60)
    models.append(model)

for model in models:
    plt.plot(np.arange(5),model.loss_values)
    
    # 调参
    # plt.plot(np.arange(30),model.loss_values)
    # plt.plot(np.arange(1,5),model.loss_values[1,6])
plt.ylabel('Train Error')
plt.xlabel('epoch')
# plt.ylim(0, 100) 

plt.legend(methods, loc='upper left')
plt.title("Train error for different GD algorithms")

plt.show()

for model in models:
    print("for "+ str(model.optimizer))
    y_pred = model.predict(dataset=test_dataset)
    y = test_dataset.labels
    print("test mean squared error: "+str(mean_squared_error(y_pred,y)))
    print("test mean absolute error: "+ str(mean_absolute_error(y_pred,y)))

"""### 优化 dropout values"""

droupouts = [0,0.1,0.5]
models_dropout=[]

for droupout in droupouts:
    dropout = [droupout,droupout]
    model = NeuralNet(
        wide_dim,
        embeddings_input,
        continuous_cols,
        deep_column_idx,
        hidden_layers,
        dropout,
        encoding_dict,
        n_class)
    model.compile(optimizer="Adam")
    if use_cuda:
        model = model.cuda()
    model.fit(dataset=train_dataset, n_epochs=3, batch_size=256)
    # 调参
    # model.fit(dataset=train_dataset, n_epochs=5, batch_size=60)
    models_dropout.append(model)

for model in models_dropout:
    plt.plot(np.arange(3),model.loss_values)

    # 调参
    # plt.plot(np.arange(5),model.loss_values)
plt.ylabel('Train Error')
plt.xlabel('epoch')

plt.legend(droupouts, loc='upper left')
plt.title("Train error for different dropouts")
plt.show()

# 直接显示平均值即可
for model in models_dropout:
    print("for drououts: "+ str(model.dropout))
    y_pred = model.predict(dataset=test_dataset)
    y = test_dataset.labels
    print("test mean squared error: "+str(mean_squared_error(y_pred,y)))
    print("test mean absolute error: "+ str(mean_absolute_error(y_pred,y)))

"""### 根据dropout训练"""

# dropout调参
# dropout = [0.5,0.5]
dropout = [0.1,0.1]
model = NeuralNet(
    wide_dim,
    embeddings_input,
    continuous_cols,
    deep_column_idx,
    hidden_layers,
    dropout,
    encoding_dict,
    n_class)
model.compile(optimizer="Adam")
if use_cuda:
    model = model.cuda()
model.fit(dataset=train_dataset, n_epochs=10, batch_size=256)
# plt.plot(np.arange(10),model.loss_values)

# 调参
plt.plot(np.arange(10),model.loss_values)
# plt.plot(np.arange(50),model.loss_values)
plt.ylabel('Train Error')
plt.xlabel('epoch')

plt.title("Train error for optimal model")
plt.show()

"""### 平均值"""

y_pred = model.predict(dataset=test_dataset)
y = test_dataset.labels
print("test mean squared error: "+str(mean_squared_error(y_pred,y)))
print("test mean absolute error: "+ str(mean_absolute_error(y_pred,y)))

"""### 计算不同子集运行模型的时间"""

# unique_users = final_df['user'].unique()
# user_quantiles = np.arange(0.1,1,0.1)
# runtimes = []

# import time
# for quantile in user_quantiles:
#     start_time = time.time()
#     subset_users = unique_users[:int(len(unique_users)*quantile)]
#     subset_df = final_df.loc[final_df['user'].isin(subset_users)]
#     data_processed = data_processing(
#         subset_df, wide_cols,
#         embeddings_cols,
#         continuous_cols,
#         target,
#         scale=True)
#     model = NeuralNet(
#         wide_dim,
#         embeddings_input,
#         continuous_cols,
#         deep_column_idx,
#         hidden_layers,
#         dropout,
#         encoding_dict,
#         n_class)
#     model.compile(optimizer='Adam')
#     if use_cuda:
#         model = model.cuda()
#     train_dataset = data_processed['train_dataset']
#     model.fit(dataset=train_dataset, n_epochs=1, batch_size=60)
#     end_time = time.time()
#     total_time = end_time- start_time
#     print("total time:" + str(total_time))
#     runtimes.append(total_time)

# plt.plot(user_quantiles,runtimes)
# plt.ylabel('Runtime')
# plt.xlabel('quantile size')

# plt.title("Runtime for optimal model for different subset size")
# plt.show()